{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, LongType, IntegerType, ArrayType, Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/13 21:25:00 WARN Utils: Your hostname, Pavans-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.29.143 instead (on interface en0)\n",
      "23/03/13 21:25:00 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "23/03/13 21:25:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"SparkDev\").config(\"spark.jars\", \"/opt/homebrew/Cellar/apache-spark/3.3.0/libexec/jars/mysql-connector-java-8.0.16.jar\").getOrCreate()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# define the structure to the data frame\n",
    "schema = StructType([\n",
    "    StructField(name=\"FirstName\", dataType=StringType(), nullable=False),\n",
    "    StructField(name=\"LastName\", dataType=StringType(), nullable=False),\n",
    "    StructField(name=\"Age\", dataType=IntegerType(), nullable=False),\n",
    "    StructField(name=\"Place\", dataType=StringType(), nullable=False),\n",
    "    StructField(name=\"Salary\", dataType=LongType(), nullable=False),\n",
    "    StructField(name=\"Department\", dataType=StringType(), nullable=False)\n",
    "])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# create the data rows as per the schema defined\n",
    "rows = [\n",
    "    Row(\"Pavan\",\"Mantha\",36,\"Hyderabad\",273567,\"SPS\"),\n",
    "    Row(\"Arun\",\"Boppudi\",36,\"Guntur\",303567,\"Aero\"),\n",
    "    Row(\"Ravi\",\"Vadlamani\",26,\"Visakapatnam\",213567,\"Aero\"),\n",
    "    Row(\"Mahender\",\"M\",21,\"Hyderabad\",153567,\"Aero\"),\n",
    "    Row(\"Manoj\",\"Manoj\",21,\"Guntur\",183567,\"Aero\"),\n",
    "    Row(\"Manoj\",\"Velecheti\",21,\"Visakapatnam\",223567,\"Aero\"),\n",
    "]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+---+------------+------+----------+\n",
      "|FirstName| LastName|Age|       Place|Salary|Department|\n",
      "+---------+---------+---+------------+------+----------+\n",
      "|    Pavan|   Mantha| 36|   Hyderabad|273567|       SPS|\n",
      "|     Arun|  Boppudi| 36|      Guntur|303567|      Aero|\n",
      "|     Ravi|Vadlamani| 26|Visakapatnam|213567|      Aero|\n",
      "| Mahender|        M| 21|   Hyderabad|153567|      Aero|\n",
      "|    Manoj|    Manoj| 21|      Guntur|183567|      Aero|\n",
      "|    Manoj|Velecheti| 21|Visakapatnam|223567|      Aero|\n",
      "+---------+---------+---+------------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parallel_rows = spark.sparkContext.parallelize(rows)\n",
    "# createDataFrame is used to create dataframe manually\n",
    "df = spark.createDataFrame(parallel_rows, schema, verifySchema=True)\n",
    "df.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading class `com.mysql.jdbc.Driver'. This is deprecated. The new driver class is `com.mysql.cj.jdbc.Driver'. The driver is automatically registered via the SPI and manual loading of the driver class is generally unnecessary.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Write to MySQL Table\n",
    "# Define database connection properties\n",
    "url = \"jdbc:mysql://localhost:3306/test\"\n",
    "table_name = \"employee_4m_spark\"\n",
    "properties = {\n",
    "    \"user\": \"root\",\n",
    "    \"password\": \"root\",\n",
    "    \"driver\": \"com.mysql.jdbc.Driver\"\n",
    "}\n",
    "\n",
    "# Write the DataFrame to the database table\n",
    "df.write.jdbc(url=url, table=table_name, mode=\"overwrite\", properties=properties)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
