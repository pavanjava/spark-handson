{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15a4bedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, LongType, IntegerType, ArrayType, Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49d5d021",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"sparksql-tutorial\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b966da58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the structure to the data frame\n",
    "schema = StructType([\n",
    "    StructField(name=\"FirstName\", dataType=StringType(), nullable=False),\n",
    "    StructField(name=\"LastName\", dataType=StringType(), nullable=False),\n",
    "    StructField(name=\"Age\", dataType=IntegerType(), nullable=False),\n",
    "    StructField(name=\"Place\", dataType=StringType(), nullable=False),\n",
    "    StructField(name=\"Salary\", dataType=LongType(), nullable=False),\n",
    "    StructField(name=\"Department\", dataType=StringType(), nullable=False),\n",
    "    StructField(name=\"Technologies\", dataType=ArrayType(elementType=StringType()), nullable=False),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4ae3b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the data rows as per the schema defined\n",
    "rows = [\n",
    "    Row(\"Pavan\",\"Mantha\",36,\"Hyderabad\",273567,\"SPS\",[\"java\",\"spring boot\",\"data science\",\"react\",\"node\", \"Terraform\"]),\n",
    "    Row(\"Arun\",\"Boppudi\",36,\"Guntur\",303567,\"Aero\",[\"java\",\"spring boot\",\"cloud\",\"react\",\"node\", \"druid\", \"kafka\"]),\n",
    "    Row(\"Ravi\",\"Vadlamani\",26,\"Visakapatnam\",213567,\"Aero\",[\"express\",\"data structures\",\"react\"]),\n",
    "    Row(\"Mahender\",\"M\",21,\"Hyderabad\",153567,\"Aero\",[\"java\",\"spring boot\",\"express\",\"react\",\"node\"]),\n",
    "    Row(\"Manoj\",\"Manoj\",21,\"Guntur\",183567,\"Aero\",[\"express\",\"react\"]),\n",
    "    Row(\"Manoj\",\"Velecheti\",21,\"Visakapatnam\",223567,\"Aero\",[\"java\",\"spring boot\",\"express\",\"react\"]),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22fd1676",
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_rows = spark.sparkContext.parallelize(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff2e07be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# createDataFrame is used to create dataframe manually\n",
    "df = spark.createDataFrame(parallel_rows, schema, verifySchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cdfa8329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method json in module pyspark.sql.readwriter:\n",
      "\n",
      "json(path: str, mode: Optional[str] = None, compression: Optional[str] = None, dateFormat: Optional[str] = None, timestampFormat: Optional[str] = None, lineSep: Optional[str] = None, encoding: Optional[str] = None, ignoreNullFields: Union[bool, str, NoneType] = None) -> None method of pyspark.sql.readwriter.DataFrameWriter instance\n",
      "    Saves the content of the :class:`DataFrame` in JSON format\n",
      "    (`JSON Lines text format or newline-delimited JSON <http://jsonlines.org/>`_) at the\n",
      "    specified path.\n",
      "    \n",
      "    .. versionadded:: 1.4.0\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    path : str\n",
      "        the path in any Hadoop supported file system\n",
      "    mode : str, optional\n",
      "        specifies the behavior of the save operation when data already exists.\n",
      "    \n",
      "        * ``append``: Append contents of this :class:`DataFrame` to existing data.\n",
      "        * ``overwrite``: Overwrite existing data.\n",
      "        * ``ignore``: Silently ignore this operation if data already exists.\n",
      "        * ``error`` or ``errorifexists`` (default case): Throw an exception if data already                 exists.\n",
      "    \n",
      "    Other Parameters\n",
      "    ----------------\n",
      "    Extra options\n",
      "        For the extra options, refer to\n",
      "        `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-json.html#data-source-option>`_\n",
      "        in the version you use.\n",
      "    \n",
      "        .. # noqa\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df.write.json(os.path.join(tempfile.mkdtemp(), 'data'))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(df.write.json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "95584c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.repartition(1).write.json('employee',mode='append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799b4890",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
