{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ACCOUNT_KEY = \"Ukbgwwg+pzkJgnz15rDQiRfWRUQdbmCz89/eUP/pIDltnbsIz0/hPRMQBQSSew+zON6VQ97ZdHrt+AStxHRYgw==\"\n",
    "CONNECTION_STR = \"DefaultEndpointsProtocol=https;AccountName=sparkpocdatastorage;AccountKey=Ukbgwwg+pzkJgnz15rDQiRfWRUQdbmCz89/eUP/pIDltnbsIz0/hPRMQBQSSew+zON6VQ97ZdHrt+AStxHRYgw==;EndpointSuffix=core.windows.net\"\n",
    "STORAGE_ACCT_NAME = \"sparkpocdatastorage\"\n",
    "CONTAINER_NAME = \"csvdatastore\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "options = {'account_key':ACCOUNT_KEY}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "df = pd.read_csv('abfs://'+CONTAINER_NAME+'@'+STORAGE_ACCT_NAME+'.dfs.core.windows.net/heart_data.csv', storage_options=options)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "   index  id    age  gender  height  weight  ap_hi  ap_lo  cholesterol  gluc  \\\n0      0   0  18393       2     168    62.0    110     80            1     1   \n1      1   1  20228       1     156    85.0    140     90            3     1   \n2      2   2  18857       1     165    64.0    130     70            3     1   \n3      3   3  17623       2     169    82.0    150    100            1     1   \n4      4   4  17474       1     156    56.0    100     60            1     1   \n\n   smoke  alco  active  cardio  \n0      0     0       1       0  \n1      0     0       1       1  \n2      0     0       0       1  \n3      0     0       1       1  \n4      0     0       0       0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>index</th>\n      <th>id</th>\n      <th>age</th>\n      <th>gender</th>\n      <th>height</th>\n      <th>weight</th>\n      <th>ap_hi</th>\n      <th>ap_lo</th>\n      <th>cholesterol</th>\n      <th>gluc</th>\n      <th>smoke</th>\n      <th>alco</th>\n      <th>active</th>\n      <th>cardio</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>18393</td>\n      <td>2</td>\n      <td>168</td>\n      <td>62.0</td>\n      <td>110</td>\n      <td>80</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>1</td>\n      <td>20228</td>\n      <td>1</td>\n      <td>156</td>\n      <td>85.0</td>\n      <td>140</td>\n      <td>90</td>\n      <td>3</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>2</td>\n      <td>18857</td>\n      <td>1</td>\n      <td>165</td>\n      <td>64.0</td>\n      <td>130</td>\n      <td>70</td>\n      <td>3</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>3</td>\n      <td>17623</td>\n      <td>2</td>\n      <td>169</td>\n      <td>82.0</td>\n      <td>150</td>\n      <td>100</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>4</td>\n      <td>17474</td>\n      <td>1</td>\n      <td>156</td>\n      <td>56.0</td>\n      <td>100</td>\n      <td>60</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/01 09:40:35 WARN Utils: Your hostname, Pavans-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.1.100 instead (on interface en0)\n",
      "23/05/01 09:40:35 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/01 09:40:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"sparkdev-adlfs-integration\").master(\"local[*]\").getOrCreate()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pavanmantha/miniconda3/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:474: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "/Users/pavanmantha/miniconda3/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:486: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n"
     ]
    }
   ],
   "source": [
    "spark_df = spark.createDataFrame(df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----+------+------+------+-----+-----+-----------+----+-----+----+------+------+\n",
      "|index|id |age  |gender|height|weight|ap_hi|ap_lo|cholesterol|gluc|smoke|alco|active|cardio|\n",
      "+-----+---+-----+------+------+------+-----+-----+-----------+----+-----+----+------+------+\n",
      "|0    |0  |18393|2     |168   |62.0  |110  |80   |1          |1   |0    |0   |1     |0     |\n",
      "|1    |1  |20228|1     |156   |85.0  |140  |90   |3          |1   |0    |0   |1     |1     |\n",
      "|2    |2  |18857|1     |165   |64.0  |130  |70   |3          |1   |0    |0   |0     |1     |\n",
      "|3    |3  |17623|2     |169   |82.0  |150  |100  |1          |1   |0    |0   |1     |1     |\n",
      "|4    |4  |17474|1     |156   |56.0  |100  |60   |1          |1   |0    |0   |0     |0     |\n",
      "+-----+---+-----+------+------+------+-----+-----+-----------+----+-----+----+------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark_df.show(5, truncate=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                        (0 + 10) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+---+------+------+------+-----+-----+-----------+----+-----+----+------+------+\n",
      "|index| id|age|gender|height|weight|ap_hi|ap_lo|cholesterol|gluc|smoke|alco|active|cardio|\n",
      "+-----+---+---+------+------+------+-----+-----+-----------+----+-----+----+------+------+\n",
      "|    0|  0|  0|     0|     0|     0|    0|    0|          0|   0|    0|   0|     0|     0|\n",
      "+-----+---+---+------+------+------+-----+-----+-----------+----+-----+----+------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Find Count of Null, None, NaN of All DataFrame Columns\n",
    "spark_df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df.columns]).show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "columns = spark_df.columns\n",
    "\n",
    "columns.remove('id')\n",
    "columns.remove('index')\n",
    "\n",
    "input_columns = columns[:11]\n",
    "target_column = columns[:-1]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "['age',\n 'gender',\n 'height',\n 'weight',\n 'ap_hi',\n 'ap_lo',\n 'cholesterol',\n 'gluc',\n 'smoke',\n 'alco',\n 'active']"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_columns"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----+------+------+------+-----+-----+-----------+----+-----+----+------+------+--------------------------------------------------------+\n",
      "|index|id |age  |gender|height|weight|ap_hi|ap_lo|cholesterol|gluc|smoke|alco|active|cardio|features                                                |\n",
      "+-----+---+-----+------+------+------+-----+-----+-----------+----+-----+----+------+------+--------------------------------------------------------+\n",
      "|0    |0  |18393|2     |168   |62.0  |110  |80   |1          |1   |0    |0   |1     |0     |[18393.0,2.0,168.0,62.0,110.0,80.0,1.0,1.0,0.0,0.0,1.0] |\n",
      "|1    |1  |20228|1     |156   |85.0  |140  |90   |3          |1   |0    |0   |1     |1     |[20228.0,1.0,156.0,85.0,140.0,90.0,3.0,1.0,0.0,0.0,1.0] |\n",
      "|2    |2  |18857|1     |165   |64.0  |130  |70   |3          |1   |0    |0   |0     |1     |[18857.0,1.0,165.0,64.0,130.0,70.0,3.0,1.0,0.0,0.0,0.0] |\n",
      "|3    |3  |17623|2     |169   |82.0  |150  |100  |1          |1   |0    |0   |1     |1     |[17623.0,2.0,169.0,82.0,150.0,100.0,1.0,1.0,0.0,0.0,1.0]|\n",
      "|4    |4  |17474|1     |156   |56.0  |100  |60   |1          |1   |0    |0   |0     |0     |[17474.0,1.0,156.0,56.0,100.0,60.0,1.0,1.0,0.0,0.0,0.0] |\n",
      "+-----+---+-----+------+------+------+-----+-----+-----------+----+-----+----+------+------+--------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vector_assembler = VectorAssembler(inputCols=input_columns, outputCol=\"features\", handleInvalid=\"skip\")\n",
    "transformed_df = vector_assembler.transform(spark_df)\n",
    "transformed_df.show(5, truncate=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------------------------------------------+\n",
      "|cardio|features                                                |\n",
      "+------+--------------------------------------------------------+\n",
      "|0     |[18393.0,2.0,168.0,62.0,110.0,80.0,1.0,1.0,0.0,0.0,1.0] |\n",
      "|1     |[20228.0,1.0,156.0,85.0,140.0,90.0,3.0,1.0,0.0,0.0,1.0] |\n",
      "|1     |[18857.0,1.0,165.0,64.0,130.0,70.0,3.0,1.0,0.0,0.0,0.0] |\n",
      "|1     |[17623.0,2.0,169.0,82.0,150.0,100.0,1.0,1.0,0.0,0.0,1.0]|\n",
      "|0     |[17474.0,1.0,156.0,56.0,100.0,60.0,1.0,1.0,0.0,0.0,0.0] |\n",
      "+------+--------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "required_df = transformed_df.drop('index', 'id', 'age', 'gender', 'height', 'weight', 'ap_hi', 'ap_lo', 'cholesterol', 'gluc', 'smoke', 'alco', 'active')\n",
    "required_df.show(5, truncate=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:>                                                        (0 + 10) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|cardio|\n",
      "+------+\n",
      "|0     |\n",
      "|1     |\n",
      "+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "required_df.select(col(\"cardio\")).distinct().show(truncate=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "(train_set, test_set) = required_df.randomSplit([0.8,0.2])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier, RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier(featuresCol='features', labelCol='cardio')\n",
    "model = dt.fit(train_set)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------------------------------------------+----------------+----------------------------------------+----------+\n",
      "|cardio|features                                               |rawPrediction   |probability                             |prediction|\n",
      "+------+-------------------------------------------------------+----------------+----------------------------------------+----------+\n",
      "|0     |[14321.0,2.0,172.0,83.0,130.0,80.0,1.0,1.0,1.0,0.0,1.0]|[1499.0,1376.0] |[0.5213913043478261,0.4786086956521739] |0.0       |\n",
      "|0     |[14333.0,2.0,172.0,81.0,120.0,80.0,1.0,1.0,0.0,0.0,1.0]|[14927.0,4255.0]|[0.7781774580335732,0.22182254196642687]|0.0       |\n",
      "|0     |[14359.0,2.0,169.0,68.0,110.0,80.0,1.0,1.0,0.0,0.0,1.0]|[14927.0,4255.0]|[0.7781774580335732,0.22182254196642687]|0.0       |\n",
      "|0     |[14373.0,1.0,169.0,67.0,120.0,80.0,1.0,1.0,0.0,0.0,1.0]|[14927.0,4255.0]|[0.7781774580335732,0.22182254196642687]|0.0       |\n",
      "|0     |[14400.0,1.0,153.0,70.0,120.0,70.0,1.0,1.0,0.0,0.0,1.0]|[14927.0,4255.0]|[0.7781774580335732,0.22182254196642687]|0.0       |\n",
      "|0     |[14413.0,1.0,156.0,61.0,130.0,70.0,2.0,2.0,0.0,0.0,1.0]|[1499.0,1376.0] |[0.5213913043478261,0.4786086956521739] |0.0       |\n",
      "|0     |[14417.0,2.0,182.0,81.0,110.0,70.0,1.0,1.0,0.0,0.0,1.0]|[14927.0,4255.0]|[0.7781774580335732,0.22182254196642687]|0.0       |\n",
      "|0     |[14421.0,1.0,156.0,67.0,110.0,70.0,1.0,2.0,0.0,0.0,1.0]|[14927.0,4255.0]|[0.7781774580335732,0.22182254196642687]|0.0       |\n",
      "|0     |[14454.0,1.0,155.0,63.0,120.0,80.0,2.0,1.0,0.0,0.0,1.0]|[14927.0,4255.0]|[0.7781774580335732,0.22182254196642687]|0.0       |\n",
      "|0     |[14454.0,1.0,169.0,70.0,120.0,70.0,1.0,2.0,0.0,0.0,1.0]|[14927.0,4255.0]|[0.7781774580335732,0.22182254196642687]|0.0       |\n",
      "+------+-------------------------------------------------------+----------------+----------------------------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = model.transform(test_set)\n",
    "predictions.show(10, truncate=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 27:>                                                       (0 + 10) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy= 0.7263979934409468\n",
      "Test Error= 0.2736020065590532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(labelCol='cardio', predictionCol='prediction')\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print('Accuracy=', accuracy)\n",
    "print('Test Error=', 1.0-accuracy)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "rfc = RandomForestClassifier(featuresCol='features',labelCol='cardio', numTrees=5)\n",
    "model_rfc = rfc.fit(train_set)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------------------------------------------+---------------------------------------+----------------------------------------+----------+\n",
      "|cardio|features                                               |rawPrediction                          |probability                             |prediction|\n",
      "+------+-------------------------------------------------------+---------------------------------------+----------------------------------------+----------+\n",
      "|0     |[14321.0,2.0,172.0,83.0,130.0,80.0,1.0,1.0,1.0,0.0,1.0]|[3.1677591179222824,1.8322408820777176]|[0.6335518235844565,0.3664481764155435] |0.0       |\n",
      "|0     |[14333.0,2.0,172.0,81.0,120.0,80.0,1.0,1.0,0.0,0.0,1.0]|[3.7710849908724042,1.2289150091275962]|[0.7542169981744808,0.24578300182551924]|0.0       |\n",
      "|0     |[14359.0,2.0,169.0,68.0,110.0,80.0,1.0,1.0,0.0,0.0,1.0]|[3.881246162177815,1.1187538378221855] |[0.7762492324355629,0.22375076756443707]|0.0       |\n",
      "|0     |[14373.0,1.0,169.0,67.0,120.0,80.0,1.0,1.0,0.0,0.0,1.0]|[3.7710849908724042,1.2289150091275962]|[0.7542169981744808,0.24578300182551924]|0.0       |\n",
      "|0     |[14400.0,1.0,153.0,70.0,120.0,70.0,1.0,1.0,0.0,0.0,1.0]|[3.9182364160559744,1.0817635839440256]|[0.7836472832111949,0.21635271678880513]|0.0       |\n",
      "+------+-------------------------------------------------------+---------------------------------------+----------------------------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions_rfc = model_rfc.transform(test_set)\n",
    "predictions_rfc.show(5, truncate=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy= 0.7286927889575063\n",
      "Test Error= 0.27130721104249367\n"
     ]
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(labelCol='cardio', predictionCol='prediction')\n",
    "accuracy = evaluator.evaluate(predictions_rfc)\n",
    "print('Accuracy=', accuracy)\n",
    "print('Test Error=', 1.0-accuracy)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# save the sparkml model\n",
    "model.save(\"decision_tree_classifier\")\n",
    "model_rfc.save(\"random_forest_classifier\")\n",
    "\n",
    "#zip the model folder\n",
    "shutil.make_archive(\"decision_tree_classifier\", \"zip\", \"decision_tree_classifier\")\n",
    "shutil.make_archive(\"random_forest_classifier\", \"zip\", \"random_forest_classifier\")\n",
    "\n",
    "#delete the actual folder\n",
    "shutil.rmtree(\"decision_tree_classifier\")\n",
    "shutil.rmtree(\"random_forest_classifier\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Upload Model back to Azure Blob Storage"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File upload complete to container trained-models\n",
      "File upload complete to container trained-models\n"
     ]
    }
   ],
   "source": [
    "from azure.storage.blob import BlobServiceClient\n",
    "\n",
    "CONTAINER_NAME = \"trained-models\"\n",
    "\n",
    "def upload_file(file_path, file_name):\n",
    "    blob_service_client = BlobServiceClient.from_connection_string(CONNECTION_STR)\n",
    "    blob_client = blob_service_client.get_blob_client(container=CONTAINER_NAME, blob=file_name)\n",
    "\n",
    "    with open(file_path, \"rb\") as file:\n",
    "        blob_client.upload_blob(file)\n",
    "\n",
    "    print(f\"File upload complete to container {CONTAINER_NAME}\")\n",
    "\n",
    "\n",
    "upload_file(\"/Users/pavanmantha/Pavans/PracticeExamples/DataScience_Practice/spark-handson/AzureBlobStorage_Spark_Integration/decision_tree_classifier.zip\", \"decision_tree_classifier.zip\")\n",
    "\n",
    "upload_file(\"/Users/pavanmantha/Pavans/PracticeExamples/DataScience_Practice/spark-handson/AzureBlobStorage_Spark_Integration/random_forest_classifier.zip\", \"random_forest_classifier.zip\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
